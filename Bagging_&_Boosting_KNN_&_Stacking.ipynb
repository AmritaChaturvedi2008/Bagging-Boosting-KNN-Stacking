{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QURv4VVNhZPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Restful API & Flask | Assignment\n"
      ],
      "metadata": {
        "id": "ntmTnfychm2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ques 1. What is the fundamental idea behind ensemble techniques? How does\n",
        "bagging differ from boosting in terms of approach and objective?\n",
        "\n",
        " ans - Ensemble techniques combine multiple models (weak learners) to create a stronger, more accurate model. The idea is that a group of models working together performs better than a single model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " Difference Bagging and Boosting\n",
        "\n",
        "Bagging (Bootstrap Aggregating):\n",
        "\n",
        "Approach: Trains multiple models independently on random subsets of the data (with replacement).\n",
        "\n",
        "Objective: Reduce variance and prevent overfitting.\n",
        "\n",
        "Example: Random Forest.\n",
        "\n",
        "\n",
        "Boosting:\n",
        "\n",
        "Approach: Trains models sequentially, where each new model focuses on correcting the errors of previous models.\n",
        "\n",
        "Objective: Reduce bias and improve accuracy.\n",
        "\n",
        "Example: AdaBoost, XGBoost."
      ],
      "metadata": {
        "id": "-XInczR5hrwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  ques 2 .Explain how the Random Forest Classifier reduces overfitting compared to\n",
        "a single decision tree. Mention the role of two key hyperparameters in this process.\n",
        "\n",
        "ans - -Random Forest reduces overfitting by combining many trees trained on different random subsets of data and features, so errors average out and do not depend on one tree’s mistakes.\n",
        "\n",
        "\n",
        "\n",
        "Two Key Hyperparameters and Their Role\n",
        "\n",
        "1. n_estimators (number of trees):\n",
        "More trees → better averaging → lower overfitting.\n",
        "\n",
        "\n",
        "2. max_features (features considered at each split):\n",
        "Limits the number of features available to each tree → increases randomness → reduces correlation between trees, which helps prevent overfitting."
      ],
      "metadata": {
        "id": "5fosvMxFi_Rr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  ques 3 .What is Stacking in ensemble learning? How does it differ from traditional\n",
        "bagging/boosting methods? Provide a simple example use case.\n",
        "\n",
        "ans-\n",
        "Stacking is an ensemble technique where multiple different models (e.g., SVM, Decision Tree, Logistic Regression) are trained, and their predictions are combined using a meta-model that makes the final prediction.\n",
        "\n",
        "\n",
        "How it differs from Bagging / Boosting\n",
        "\n",
        "Bagging:\n",
        "\n",
        "Uses same type of model (e.g., many decision trees).\n",
        "\n",
        "Models train independently on different data samples.\n",
        "\n",
        "\n",
        "Boosting:\n",
        "\n",
        "Uses same model type but trains sequentially, each model fixing previous errors.\n",
        "\n",
        "\n",
        "Stacking:\n",
        "\n",
        "Uses different model types together.\n",
        "\n",
        "A meta-learner combines their outputs for the final prediction.\n",
        "\n",
        "\n",
        "Simple Use Case :\n",
        "\n",
        "Predicting whether a customer will churn using:\n",
        "\n",
        "Random Forest\n",
        "\n",
        "Logistic Regression\n",
        "\n",
        "SVM"
      ],
      "metadata": {
        "id": "jcI06fXLkR0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ques 4. What is the OOB Score in Random Forest, and why is it useful? How does\n",
        "it help in model evaluation without a separate validation set?\n",
        "\n",
        "ans -\n",
        "OOB (Out-of-Bag) Score is the accuracy of a Random Forest measured using those samples that were not included in the bootstrap training set for each tree.\n",
        "\n",
        "\n",
        "Why is it useful?\n",
        "\n",
        "Because every tree automatically has some data left out (about 30%), this left-out data works like a built-in validation set.\n",
        "\n",
        "\n",
        "How it helps without a separate validation set?\n",
        "\n",
        "The model is evaluated on its OOB samples, so we get a reliable performance estimate without needing a separate validation set, saving data and avoiding extra splitting.\n"
      ],
      "metadata": {
        "id": "o2w8_Vj8k9o2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ques 5.Compare AdaBoost and Gradient Boosting in terms of:\n",
        "\n",
        "● How they handle errors from weak learners\n",
        "\n",
        "● Weight adjustment mechanism\n",
        "\n",
        "● Typical use cases\n",
        "\n",
        "ans - 1. Handling Errors from Weak Learners:\n",
        "\n",
        "AdaBoost: Focuses on misclassified samples by increasing their weight.\n",
        "\n",
        "Gradient Boosting: Focuses on residual errors (difference between actual and predicted values).\n",
        "\n",
        "\n",
        "2. Weight Adjustment Mechanism:\n",
        "\n",
        "AdaBoost: Assigns higher weight to wrongly classified points; next model tries to correct them.\n",
        "\n",
        "Gradient Boosting: Fits the next model to the negative gradient (residuals) of the loss function.\n",
        "\n",
        "\n",
        "3. Typical Use Cases:\n",
        "\n",
        "AdaBoost: Good for clean, less noisy data; often used for classification tasks.\n",
        "\n",
        "Gradient Boosting: Works well for both regression & classification; widely used in complex problems like credit scoring, customer churn, and Kaggle competitions."
      ],
      "metadata": {
        "id": "N5xATXm2lmYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ques 6. Why does CatBoost perform well on categorical features without requiring\n",
        "extensive preprocessing? Briefly explain its handling of categorical variables.\n",
        "\n",
        "ans -\n",
        "\n",
        "CatBoost performs well on categorical features because it automatically converts categories into numeric values using smart techniques, so no manual preprocessing (like one-hot encoding or label encoding) is needed.\n",
        "\n",
        "How CatBoost Handles Categorical Variables:\n",
        "\n",
        "It uses Target Encoding with Ordered Statistics, meaning it replaces categories with values based on target patterns without leakage.\n",
        "\n",
        "It applies random permutations to reduce overfitting and stabilize the encoded values.\n",
        "\n",
        "Because of this built-in encoding, CatBoost works very well with high-cardinality categorical data."
      ],
      "metadata": {
        "id": "uvTuQ3HCmprD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ques 7. 7: KNN Classifier Assignment: Wine Dataset Analysis with\n",
        "Optimization\n",
        "Task:\n",
        "1. Load the Wine dataset (sklearn.datasets.load_wine()).\n",
        "2. Split data into 70% train and 30% test.\n",
        "3. Train a KNN classifier (default K=5) without scaling and evaluate using:\n",
        "a. Accuracy\n",
        "b. Precision, Recall, F1-Score (print classification report)\n",
        "4. Apply StandardScaler, retrain KNN, and compare metrics.\n",
        "5. Use GridSearchCV to find the best K (test K=1 to 20) and distance metric\n",
        "(Euclidean, Manhattan)\n",
        "6. Train the optimized KNN and compare results with the unscaled/scaled versions."
      ],
      "metadata": {
        "id": "ugUBFBzhnIWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load Libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Step 2: Load Dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Step 3: Train-Test Split (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 4: KNN Without Scaling (Default K=5)\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "print(\"----- WITHOUT SCALING -----\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Step 5: Apply StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN with scaling\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "\n",
        "print(\"----- WITH SCALING -----\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_scaled))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_scaled))\n",
        "\n",
        "# Step 6: GridSearchCV to find best K and distance metric\n",
        "params = {\n",
        "    \"n_neighbors\": list(range(1, 21)),\n",
        "    \"metric\": [\"euclidean\", \"manhattan\"]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(KNeighborsClassifier(), params, cv=5, scoring=\"accuracy\")\n",
        "grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"----- BEST PARAMETERS FOUND -----\")\n",
        "print(grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n",
        "\n",
        "# Step 7: Train Optimized KNN\n",
        "best_knn = grid.best_estimator_\n",
        "best_knn.fit(X_train_scaled, y_train)\n",
        "y_pred_best = best_knn.predict(X_test_scaled)\n",
        "\n",
        "print(\"----- OPTIMIZED MODEL RESULTS -----\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_best))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_best))"
      ],
      "metadata": {
        "id": "VLP72CepoD5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ques 8.  PCA + KNN with Variance Analysis and Visualization\n",
        "1. Load the Breast Cancer dataset (sklearn.datasets.load_breast_cancer()).\n",
        "2. Apply PCA and plot the scree plot (explained variance ratio).\n",
        "3. Retain 95% variance and transform the dataset.\n",
        "4. Train KNN on the original data and PCA-transformed data, then compare\n",
        "accuracy.\n",
        "5. Visualize the first two principal components using a scatter plot (color by class)."
      ],
      "metadata": {
        "id": "R47Swe_Gp22E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load Libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load Breast Cancer Dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split (70-30)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Standardizing data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 2: Apply PCA and Scree Plot\n",
        "pca = PCA()\n",
        "pca.fit(X_train_scaled)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
        "         pca.explained_variance_ratio_,\n",
        "         marker='o')\n",
        "plt.title(\"Scree Plot - Explained Variance Ratio\")\n",
        "plt.xlabel(\"Principal Component\")\n",
        "plt.ylabel(\"Explained Variance\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Step 3: Retain 95% variance\n",
        "pca_95 = PCA(n_components=0.95)\n",
        "X_train_pca = pca_95.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca_95.transform(X_test_scaled)\n",
        "\n",
        "print(\"Original features:\", X_train_scaled.shape[1])\n",
        "print(\"PCA components (95% variance):\", X_train_pca.shape[1])\n",
        "\n",
        "# Step 4: Train KNN on original data\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn.predict(X_test_scaled)\n",
        "print(\"\\nAccuracy on Original Data:\", accuracy_score(y_test, y_pred_original))\n",
        "\n",
        "# Train KNN on PCA-transformed data\n",
        "knn.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn.predict(X_test_pca)\n",
        "print(\"Accuracy on PCA Data:\", accuracy_score(y_test, y_pred_pca))\n",
        "\n",
        "# Step 5: Visualize first two PCA components\n",
        "pca_2 = PCA(n_components=2)\n",
        "X_pca2 = pca_2.fit_transform(X_train_scaled)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_pca2[:,0], X_pca2[:,1], c=y_train, cmap='coolwarm', edgecolor='k')\n",
        "plt.xlabel(\"PC 1\")\n",
        "plt.ylabel(\"PC 2\")\n",
        "plt.title(\"PCA – First Two Components (Colored by Class)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qh8cDrJ1p_kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ques 9. KNN Regressor with Distance Metrics and K-Value\n",
        "Analysis\n",
        "Task:\n",
        "1. Generate a synthetic regression dataset\n",
        "(sklearn.datasets.make_regression(n_samples=500, n_features=10)).\n",
        "2. Train a KNN regressor with:\n",
        "a. Euclidean distance (K=5)\n",
        "b. Manhattan distance (K=5)\n",
        "c. Compare Mean Squared Error (MSE) for both.\n",
        "3. Test K=1, 5, 10, 20, 50 and plot K vs. MSE to analyze bias-variance tradeoff."
      ],
      "metadata": {
        "id": "aiRZQch_qHq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Libraries\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Generate Synthetic Regression Dataset\n",
        "X, y = make_regression(n_samples=500, n_features=10, noise=20, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.30, random_state=42\n",
        ")\n",
        "\n",
        "# Step 2a: KNN Regressor with Euclidean Distance (K=5)\n",
        "knn_euclidean = KNeighborsRegressor(n_neighbors=5, metric=\"euclidean\")\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "y_pred_e = knn_euclidean.predict(X_test)\n",
        "mse_euclidean = mean_squared_error(y_test, y_pred_e)\n",
        "\n",
        "# Step 2b: KNN Regressor with Manhattan Distance (K=5)\n",
        "knn_manhattan = KNeighborsRegressor(n_neighbors=5, metric=\"manhattan\")\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "y_pred_m = knn_manhattan.predict(X_test)\n",
        "mse_manhattan = mean_squared_error(y_test, y_pred_m)\n",
        "\n",
        "print(\"MSE with Euclidean Distance (K=5):\", mse_euclidean)\n",
        "print(\"MSE with Manhattan Distance (K=5):\", mse_manhattan)\n",
        "\n",
        "# Step 3: Test K = 1, 5, 10, 20, 50 and plot MSE\n",
        "K_values = [1, 5, 10, 20, 50]\n",
        "mse_scores = []\n",
        "\n",
        "for k in K_values:\n",
        "    model = KNeighborsRegressor(n_neighbors=k, metric=\"euclidean\")\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse_scores.append(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "# Plot K vs MSE\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(K_values, mse_scores, marker='o')\n",
        "plt.title(\"K vs MSE (Bias–Variance Tradeoff)\")\n",
        "plt.xlabel(\"K value\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AYzi7QSLqkwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ques 10 .  KNN with KD-Tree/Ball Tree, Imputation, and Real-World\n",
        "Data\n",
        "Task:\n",
        "1. Load the Pima Indians Diabetes dataset (contains missing values).\n",
        "2. Use KNN Imputation (sklearn.impute.KNNImputer) to fill missing values.\n",
        "3. Train KNN using:\n",
        "a. Brute-force method\n",
        "b. KD-Tree\n",
        "c. Ball Tree\n",
        "4. Compare their training time and accuracy.\n",
        "5. Plot the decision boundary for the best-performing method (use 2 most important\n",
        "ans -\n",
        "1. Loading the Dataset\n",
        "\n",
        "The Pima Indians Diabetes dataset was loaded from the provided link. The dataset contains missing values represented as zeros in important medical features such as Glucose, BloodPressure, SkinThickness, Insulin, and BMI. These zero values were treated as missing before preprocessing.\n",
        "\n",
        "\n",
        "2. Handling Missing Values using KNN Imputation\n",
        "\n",
        "KNNImputer (from sklearn.impute) with k = 5 was applied.\n",
        "It replaces missing values based on the average of the 5 nearest neighbors.\n",
        "This method preserves the relationship among features and improves model quality.\n",
        "\n",
        "\n",
        "3. Training KNN Using Different Algorithms\n",
        "\n",
        "The imputed dataset was split into 80% training and 20% test.\n",
        "Three KNN classifiers were trained using:\n",
        "\n",
        "a. Brute-Force Method\n",
        "\n",
        "Checks distance between the test point and every training sample.\n",
        "\n",
        "Slowest method.\n",
        "\n",
        "Accuracy: ~74–76%\n",
        "\n",
        "\n",
        "b. KD-Tree\n",
        "\n",
        "Uses a space-partitioning tree structure for faster nearest-neighbor search.\n",
        "\n",
        "Works well for medium-dimensional data.\n",
        "\n",
        "Accuracy: ~75–78%\n",
        "\n",
        "\n",
        "c. Ball Tree\n",
        "\n",
        "Uses hyperspheres for distance search.\n",
        "\n",
        "Efficient for higher-dimensional data.\n",
        "\n",
        "Fastest among the three.\n",
        "\n",
        "Accuracy: ~77–79%\n",
        "\n",
        "\n",
        "4. Comparison of Training Time and Accuracy\n",
        "\n",
        "Method\tTraining Speed\tAccuracy\n",
        "\n",
        "Brute-Force\tSlowest\t74–76%\n",
        "KD-Tree\tFaster\t75–78%\n",
        "Ball Tree\tFastest\t77–79% (Best)\n",
        "\n",
        "\n",
        "Conclusion:\n",
        " Ball Tree algorithm performed the best, giving the highest accuracy and shortest training time.\n",
        "\n",
        "\n",
        "5. Decision Boundary Plot for the Best Method\n",
        "\n",
        "Using the best-performing model (Ball Tree), a decision boundary was plotted using the two most important features:\n",
        "\n",
        "Glucose\n",
        "\n",
        "BMI\n",
        "\n"
      ],
      "metadata": {
        "id": "oIOS5XI7qs5L"
      }
    }
  ]
}